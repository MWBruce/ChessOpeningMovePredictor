{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 15:37:00.907250: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-13 15:37:01.079660: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-13 15:37:02.862626: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-13 15:37:04.539801: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-13 15:37:05.891891: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-13 15:37:06.221672: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-13 15:37:10.080803: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-13 15:37:49.625495: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import chess.pgn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import chess\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import LeakyReLU, Dropout\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/games.csv')\n",
    "\n",
    "# Dict to convert UCI -> Int and Int -> UCI\n",
    "uci_to_int = {}\n",
    "int_to_uci = {}\n",
    "counter = 1 \n",
    "\n",
    "# Convert UCI -> Int Move\n",
    "def move_to_int(move):\n",
    "    if move not in uci_to_int:\n",
    "        global counter\n",
    "        uci_to_int[move] = counter\n",
    "        int_to_uci[counter] = move\n",
    "        counter += 1\n",
    "    return uci_to_int[move]\n",
    "\n",
    "# Convert a string of moves into ints using the chess library\n",
    "def parse_moves(moves_str):\n",
    "    board = chess.Board()\n",
    "    move_list = []\n",
    "    for move in moves_str.split():\n",
    "        try:\n",
    "            uci_move = board.push_san(move).uci()\n",
    "            move_list.append(move_to_int(uci_move))\n",
    "        except ValueError:\n",
    "            print(f\"Invalid move: {move}\")\n",
    "            break\n",
    "    return move_list\n",
    "\n",
    "data['parsed_moves'] = data['moves'].apply(parse_moves)\n",
    "\n",
    "X = pad_sequences(data['parsed_moves'], maxlen=28, padding='post', truncating='post')\n",
    "\n",
    "## Finding out how many moves we have (I think 27)\n",
    "max_index = max(uci_to_int.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class sizes in training set before balancing:\n",
      "opening_encoded\n",
      "24    1253\n",
      "22     529\n",
      "25     528\n",
      "23     309\n",
      "0      206\n",
      "5      168\n",
      "4      148\n",
      "17     126\n",
      "16     117\n",
      "21     112\n",
      "26     105\n",
      "3       91\n",
      "20      65\n",
      "6       52\n",
      "18      45\n",
      "2       40\n",
      "15      30\n",
      "19      29\n",
      "1       25\n",
      "8       10\n",
      "13       7\n",
      "10       7\n",
      "11       3\n",
      "9        2\n",
      "12       2\n",
      "14       2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class sizes in training set after balancing:\n",
      "y\n",
      "24    1253\n",
      "22    1253\n",
      "25    1253\n",
      "23    1253\n",
      "0     1253\n",
      "5     1253\n",
      "4     1253\n",
      "17    1253\n",
      "16    1253\n",
      "21    1253\n",
      "26    1253\n",
      "3     1253\n",
      "20    1253\n",
      "6     1253\n",
      "18    1253\n",
      "2     1253\n",
      "15    1253\n",
      "19    1253\n",
      "1     1253\n",
      "8     1253\n",
      "13    1253\n",
      "10    1253\n",
      "11    1253\n",
      "9     1253\n",
      "12    1253\n",
      "14    1253\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## To avoid alot of the same name ones we cluster into just those with the same name rather than the eco\n",
    "grouped_eco_labels = {\n",
    "    'A00': 'Polish (Sokolsky) opening',\n",
    "    'A01': 'Nimzovich-Larsen attack',\n",
    "    'A02-A03': \"Bird's opening\",\n",
    "    'A04-A09': 'Reti opening',\n",
    "    'A10-A39': 'English opening',\n",
    "    'A40-A44': \"Queen's pawn\",\n",
    "    'A45-A46': \"Queen's pawn game\",\n",
    "    'A47': \"Queen's Indian defence\",\n",
    "    'A48-A49': \"King's Indian defence\",\n",
    "    'A50': \"Queen's pawn game\",\n",
    "    'A51-A52': 'Budapest defence',\n",
    "    'A53-A55': 'Old Indian defence',\n",
    "    'A56': 'Benoni defence',\n",
    "    'A57-A59': 'Benko gambit',\n",
    "    'A60-A79': 'Benoni defence',\n",
    "    'A80-A99': 'Dutch',\n",
    "    'B00': \"King's pawn opening\",\n",
    "    'B01': 'Scandinavian (centre counter) defence',\n",
    "    'B02-B05': \"Alekhine's defence\",\n",
    "    'B06': 'Robatsch (modern) defence',\n",
    "    'B07-B09': 'Pirc defence',\n",
    "    'B10-B19': 'Caro-Kann defence',\n",
    "    'B20-B99': 'Sicilian defence',\n",
    "    'C00-C19': 'French defence',\n",
    "    'C20-C99': \"King's pawn game\",\n",
    "    'D00-D99': \"Queen's Gambit\",\n",
    "    'E00-E99': \"King's Indian defence\",\n",
    "}\n",
    "\n",
    "## Converting the eco to integers\n",
    "eco_to_int = {}\n",
    "int_to_opening = {}\n",
    "counter = 0\n",
    "\n",
    "for eco_range, opening_name in grouped_eco_labels.items():\n",
    "    if '-' in eco_range:\n",
    "        start, end = eco_range.split('-')\n",
    "        for i in range(int(start[1:]), int(end[1:]) + 1):\n",
    "            eco = start[0] + str(i).zfill(2)\n",
    "            if eco not in eco_to_int:\n",
    "                eco_to_int[eco] = counter\n",
    "    else:\n",
    "        if eco_range not in eco_to_int:\n",
    "            eco_to_int[eco_range] = counter\n",
    "\n",
    "    int_to_opening[counter] = opening_name\n",
    "    counter += 1\n",
    "\n",
    "data['opening_encoded'] = data['opening_eco'].map(eco_to_int)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, data['opening_encoded'], test_size=0.8, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Balancing classes in the training set\n",
    "class_counts = pd.Series(y_train).value_counts()\n",
    "max_class_size = class_counts.max()\n",
    "\n",
    "# Print sizes before balancing\n",
    "print(\"Class sizes in training set before balancing:\")\n",
    "print(class_counts)\n",
    "\n",
    "balanced_train_data = []\n",
    "for class_label in class_counts.index:\n",
    "    class_data = pd.DataFrame({'X': list(X_train[y_train == class_label]), 'y': y_train[y_train == class_label]})\n",
    "    \n",
    "    # Tile the data\n",
    "    tiles = max_class_size // len(class_data)\n",
    "    remainder = max_class_size % len(class_data)\n",
    "    \n",
    "    tiled_data = pd.concat([class_data] * tiles)\n",
    "    \n",
    "    # Upsample the rest if remainder is greater than 0\n",
    "    if remainder > 0:\n",
    "        upsampled_data = resample(class_data, replace=True, n_samples=remainder, random_state=42)\n",
    "        balanced_class_data = pd.concat([tiled_data, upsampled_data])\n",
    "    else:\n",
    "        balanced_class_data = tiled_data\n",
    "    \n",
    "    balanced_train_data.append(balanced_class_data)\n",
    "\n",
    "# Combine all balanced training data\n",
    "balanced_train_data = pd.concat(balanced_train_data)\n",
    "\n",
    "# Print sizes after balancing\n",
    "print(\"\\nClass sizes in training set after balancing:\")\n",
    "print(balanced_train_data['y'].value_counts())\n",
    "\n",
    "# Separate X and y for the balanced training data\n",
    "X_train_balanced = np.vstack(balanced_train_data['X'].values)\n",
    "y_train_balanced = balanced_train_data['y'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prints the validation progress to a png file every \"save_every\" epochs\n",
    "class ValidationLossPlotter(Callback):\n",
    "    def __init__(self, save_every=5):\n",
    "        super(ValidationLossPlotter, self).__init__()\n",
    "        self.epoch_count = 0\n",
    "        self.save_every = save_every\n",
    "        self.history = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss')\n",
    "        self.history.append(val_loss)\n",
    "        self.epoch_count += 1\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, self.epoch_count + 1), self.history, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Validation Loss')\n",
    "        plt.title('Validation Loss Progress')\n",
    "        plt.legend()\n",
    "\n",
    "        if self.epoch_count % self.save_every == 0:\n",
    "            plt.savefig('validation_progress.png')\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "best_model_name = 'best_model.keras'\n",
    "validation_plotter = ValidationLossPlotter(save_every=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating CVAE\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(28,)),\n",
    "            tf.keras.layers.Embedding(input_dim=max_index + 1, output_dim=64, input_length=28),\n",
    "            tf.keras.layers.Conv1D(32, 3, activation='linear', padding='same'),\n",
    "            LeakyReLU(alpha=0.3),\n",
    "            tf.keras.layers.Conv1D(64, 3, activation='linear', padding='same'),\n",
    "            LeakyReLU(alpha=0.3),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='linear'),\n",
    "            LeakyReLU(alpha=0.3),\n",
    "            Dropout(0.2),\n",
    "            tf.keras.layers.Dense(256, activation='linear'),\n",
    "            LeakyReLU(alpha=0.3),\n",
    "            Dropout(0.2),\n",
    "            tf.keras.layers.Dense(latent_dim + latent_dim)\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(256, activation='linear'),\n",
    "            LeakyReLU(alpha=0.3),\n",
    "            Dropout(0.2),\n",
    "            tf.keras.layers.Dense(128, activation='linear'),\n",
    "            LeakyReLU(alpha=0.3),\n",
    "            Dropout(0.2),\n",
    "            tf.keras.layers.Dense(units=28 * max_index, activation='linear'),\n",
    "            LeakyReLU(alpha=0.3),\n",
    "            tf.keras.layers.Reshape(target_shape=(28, max_index)),\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * 0.5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis)\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "    cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def validate_step(model, x):\n",
    "    val_loss = compute_loss(model, x)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Maxwell Bruce/Desktop/AML/ChessOpenningMovePredictor/myenv/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Maxwell Bruce/Desktop/AML/ChessOpenningMovePredictor/myenv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Maxwell Bruce/Desktop/AML/ChessOpenningMovePredictor/myenv/lib/python3.10/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "2024-08-13 15:40:05.264318: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-08-13 15:40:08.546986: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 210.31668090820312, Validation Loss: 209.23435974121094\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "latent_dim = 24\n",
    "\n",
    "model = CVAE(latent_dim)\n",
    "\n",
    "batch_size = 1024\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(X_train_balanced).shuffle(len(X_train_balanced)).batch(batch_size)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(X_val).batch(batch_size)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "## Training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_val_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "    for train_x_batch in train_dataset:\n",
    "        loss = train_step(model, train_x_batch, optimizer)\n",
    "        epoch_loss_avg.update_state(loss)\n",
    "\n",
    "    # Validation loop\n",
    "    for val_x_batch in validation_dataset:\n",
    "        val_loss = validate_step(model, val_x_batch)\n",
    "        epoch_val_loss_avg.update_state(val_loss)\n",
    "\n",
    "    # Get the average loss for this epoch\n",
    "    train_loss = epoch_loss_avg.result().numpy()\n",
    "    val_loss = epoch_val_loss_avg.result().numpy()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    validation_plotter.on_epoch_end(epoch, logs={'val_loss': val_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gets the latent representation, basically meaning the generation aspect is cut off\n",
    "def get_latent_vectors(model, data):\n",
    "    latent_vectors = []\n",
    "    for sample in data:\n",
    "        mean, logvar = model.encode(sample[np.newaxis, :])\n",
    "        z = model.reparameterize(mean, logvar)\n",
    "        latent_vectors.append(z.numpy().squeeze())\n",
    "    return np.array(latent_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_latent = get_latent_vectors(model, X_train_balanced)\n",
    "X_test_latent = get_latent_vectors(model, X_test)\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_latent = imputer.fit_transform(X_train_latent)\n",
    "X_test_latent = imputer.transform(X_test_latent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.06      0.06       395\n",
      "           1       0.01      0.05      0.01        64\n",
      "           2       0.01      0.03      0.01        63\n",
      "           3       0.03      0.05      0.03       216\n",
      "           4       0.03      0.05      0.04       250\n",
      "           5       0.03      0.04      0.03       333\n",
      "           6       0.00      0.01      0.00       133\n",
      "           7       1.00      0.00      0.00         3\n",
      "           8       0.00      0.03      0.01        29\n",
      "           9       0.00      0.14      0.01         7\n",
      "          10       0.00      0.00      0.00        17\n",
      "          11       0.00      0.00      0.00         8\n",
      "          12       0.00      0.00      0.00         6\n",
      "          13       0.00      0.00      0.00         8\n",
      "          14       0.00      0.00      0.00         6\n",
      "          15       0.01      0.05      0.02        55\n",
      "          16       0.04      0.04      0.04       265\n",
      "          17       0.03      0.03      0.03       296\n",
      "          18       0.02      0.06      0.03        77\n",
      "          19       0.01      0.05      0.02        66\n",
      "          20       0.02      0.03      0.02       143\n",
      "          21       0.02      0.02      0.02       227\n",
      "          22       0.16      0.04      0.06      1020\n",
      "          23       0.07      0.03      0.04       577\n",
      "          24       0.29      0.03      0.05      2477\n",
      "          25       0.12      0.02      0.04      1067\n",
      "          26       0.03      0.03      0.03       216\n",
      "\n",
      "    accuracy                           0.03      8024\n",
      "   macro avg       0.07      0.03      0.02      8024\n",
      "weighted avg       0.14      0.03      0.04      8024\n",
      "\n",
      "[[ 24  25  17  20  15  22  25   0   6  25  20  13  10  15  10  17  10  16\n",
      "    9  13  13   7  13  11  15  12  12]\n",
      " [  5   3   1   2   3   3   3   0   1   2   6   4   1   6   2   1   1   2\n",
      "    2   2   1   1   0   2   1   1   8]\n",
      " [  2   2   2   0   4   3   3   0   1   2   5   1   3   1   4   4   4   2\n",
      "    3   1   1   3   1   4   5   1   1]\n",
      " [ 12  11   9  10   5  12  14   0   7   3  10  11   8  10   8   7  14   4\n",
      "    8   9   6  11   3   7   7   6   4]\n",
      " [ 15  12  15  13  12   7   7   0   8  12   8  16   8   9  11  10   5   9\n",
      "   13   3   7   5   8   9  13   8   7]\n",
      " [ 16  14  16  26  25  12  12   0  21  11   9  14  11  11  10  15   6  12\n",
      "    8   6  11  13  10  12  14  10   8]\n",
      " [  8   6   9  13  11   6   1   0   4   3   4   7   5   5   1   3   3   4\n",
      "    7   5   5   4   5   2   5   3   4]\n",
      " [  0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   1   0   0   1   0   0   0   0]\n",
      " [  4   0   2   1   1   1   1   0   1   1   0   2   2   0   1   0   1   0\n",
      "    2   1   1   3   1   1   0   1   1]\n",
      " [  0   1   0   0   0   0   0   0   0   1   1   0   0   1   0   0   0   0\n",
      "    0   0   0   1   1   0   1   0   0]\n",
      " [  2   1   1   2   1   1   0   0   0   1   0   0   0   0   0   0   0   0\n",
      "    1   1   0   1   2   2   0   0   1]\n",
      " [  1   1   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   1   0   0   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0   1   1   1   0   0\n",
      "    0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   1   0   1   0   0   0   0   0   1   0   0   0   1   0   1   0\n",
      "    0   0   0   0   0   1   1   1   0]\n",
      " [  0   0   1   0   3   0   1   0   0   0   0   0   0   0   0   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0]\n",
      " [  2   4   2   4   2   1   1   0   3   2   2   2   1   3   2   3   7   3\n",
      "    1   1   1   1   4   1   1   1   0]\n",
      " [ 10  16  13  14  11  14   7   0   8   6   9  13  12   9   8   8  11   9\n",
      "    9   8   9  13   8  10  11   6  13]\n",
      " [ 14  17  12  17  18  12   9   0   8  15  12  13  10   7  10  10   6   9\n",
      "    9   8  11  12  12  12   7  11  15]\n",
      " [  4   4   2   7   6   1   3   0   4   4   4   2   4   0   2   4   1   4\n",
      "    5   4   1   3   5   0   0   2   1]\n",
      " [  4   3   0   3   2   4   6   0   1   3   2   3   5   3   2   0   5   2\n",
      "    1   3   2   5   1   0   2   3   1]\n",
      " [ 11   4   5   8   8  11   3   0   4   6   4   7   3   6   8   5   6   6\n",
      "    2   5   5   2   5   2   7   6   4]\n",
      " [ 14  15  19   9   8  10  14   0   7  13  11   4  18   7  11   1   6   6\n",
      "    6   9   4   5   7   4   7   7   5]\n",
      " [ 44  54  48  52  44  44  45   0  47  46  44  30  32  29  45  24  36  23\n",
      "   50  31  35  42  38  39  29  33  36]\n",
      " [ 26  27  33  23  27  26  28   0  22  24  25  18  15  23  26  18  26  17\n",
      "   23  26  19  16  18  19  20  21  11]\n",
      " [146 136 119 108  96 119 106   0  97  88 105  97  91  82 101  89  86 102\n",
      "   93  78  94  82  63  97  73  56  73]\n",
      " [ 50  50  57  43  52  48  43   0  43  33  49  47  52  38  44  42  46  46\n",
      "   44  31  29  38  26  29  25  26  36]\n",
      " [ 17  14   6   7   5   9   7   0  16   8   8   8  11  10  12   9   9   2\n",
      "    3   7   3   5   9   7   8   9   7]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## Runs on random forest\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "evaluate_model(rf_model, X_train_latent, y_train_balanced, X_test_latent, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
